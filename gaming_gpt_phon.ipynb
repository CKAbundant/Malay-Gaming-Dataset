{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate GPT Phonemes\n",
    "\n",
    "While we have sucessfully generate text-audio pair for Malay gaming transcripts that are generated from ChatGPT 3.5, it would be good to come up with a process to convert GPT transcripts into phonemes, which will be able to be used for training via vanilla VITS model.\n",
    "\n",
    "### Our considerations:\n",
    "\n",
    "1. Phonemization by espeak-en are different from eng_to_ipa despite both being IPA phonemes. For example:\n",
    "\n",
    "<style>\n",
    "    table {\n",
    "        width: 30%;\n",
    "        table-layout:fixed;\n",
    "    }\n",
    "    th:nth-child(1),\n",
    "    td:nth-child(1) {\n",
    "        width: 33%;\n",
    "    }\n",
    "    th:nth-child(2),\n",
    "    td:nth-child(2) {\n",
    "        width: 33%;\n",
    "    }\n",
    "    th:nth-child(3),\n",
    "    td:nth-child(3) {\n",
    "        width: 34%;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "| Word| eng_to_ipa | epeak-en |\n",
    "| -- | -- | -- |\n",
    "| games | geɪmz | ɡˈeɪmz |\n",
    "| as | ɛz | æz |\n",
    "| a | ə | ɐ |\n",
    "| product | ˈprɑdəkt | pɹˈɑːdʌkt |\n",
    "| service | ˈsərvɪs | sˈɜːvɪs |\n",
    "\n",
    "2. Ideally, we should standardize the use of phonemizer so as to ensure consistency. However, we have shown that phonemes generated by both epseak-en and espeak-ms cannot produce the desired audio files.\n",
    "\n",
    "3. We noted that eng_to_ipa cannot handle heteroymns and has a default sound for a specific word/character. This is an issue since we are not able to discern the right phonemes for customizing phonemes:\n",
    "\n",
    "    - E.g. 'a' is defaulted to 'er' sound rather than 'eh' sound. Hence, phonemes generated for acronyms for 'w a s d' would be incorrect.\n",
    "    - Nevertheless, we will maintain our customized phonemes in `phon_mapping` for users to consider.\n",
    "\n",
    "4. The slight differences in phonemes may be useful to produce the intonation required for Malay accent. However, we will need more synthetic data sample (at least 10000) for VITS model to learn the differences.\n",
    "\n",
    "5. Nevertheless, we will first normalize text transcript specifically for phonemization via eng_to_ipa and espeak-en i.e. create required mapping to map English words to word combinations and generate customized phonemes if deemed fit.\n",
    "\n",
    "### Our intentions:\n",
    "\n",
    "There are 3 methods to generate phonemes list for VITS training but we chose [espeak_eng2ipa_ms_phon](#espeak_eng2ipa_ms_phon). Reasons are provided below\n",
    "\n",
    "<style>\n",
    "    table {\n",
    "        width: 70%;\n",
    "        table-layout:fixed;\n",
    "    }\n",
    "    th:nth-child(1),\n",
    "    td:nth-child(1) {\n",
    "        width: 3%;\n",
    "    }\n",
    "    th:nth-child(2),\n",
    "    td:nth-child(2) {\n",
    "        width: 17%;\n",
    "    }\n",
    "    th:nth-child(3),\n",
    "    td:nth-child(3) {\n",
    "        width: 80%;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "| S/N | Phoneme List | Desciption | \n",
    "| -- | -- | -- |\n",
    "| 1. | [espeak_ms_phon](#espeak_ms_phon) | &bull; Entire GPT corpus is phonemized by espeak-ms. <br>&bull; Unfortunately, all numbers will be converted to Malay version, which are different from the GPT text-audio pairs. |\n",
    "| 2. | [espeak_en_ms_phon](#espeak_en_ms_phon) | &bull; English words are phonemized by espeak-en. <br>&bull; Malay words are phonemized by espeak-ms. <br>&bull; This method would be ideal provided if polyglot is able to distinguish between English and Malay words accurately, which is not the case. |\n",
    "| 3. | [espeak_eng2ipa_ms_phon](#espeak_eng2ipa_ms_phon) | &bull; English words are phonemized by eng_to_ipa. <br>&bull; Malay words and English words that are not phonemized by eng_to_ipa are phonemized by espeak-ms. <br>&bull; eng_to_ipa can only phonemize proper English words and return the original word with appended with asterick at the end if it can't. |\n",
    "\n",
    "Note that:\n",
    "\n",
    "1. There should be no unphonemized words since espeak-ms is able to handle any outlier words as shown in earlier section although we are not able to verify the accuracy of the phonemes.\n",
    "2. We implement a work around for using espeak-en on English words and espeak-ms on Malay words using eng_to_ipa:\n",
    "\n",
    "    - Since eng_to_ipa will phonemize English words and append asterick for non-English words, we will use eng_to_ipa to check if phonemization is successful. If so, we use espeak-en to phonemize instead.\n",
    "    - If phonemization isn't successful, eng_to_ipa will tag the word with asterick. Therefore, we are able to identify non-English words and phonemize via espeak-ms. \n",
    "\n",
    "Steps are as follow:\n",
    "\n",
    "| S/N | Step | Desciption | \n",
    "| -- | -- | -- |\n",
    "| 1. | [Create mapping](#create_mapping) | &bull; Create `mapping_dict` for phonemization from `mapping` in `gaming.yaml`. <br>&bull; Remove duplicate keys in `gpt_mapping` that are also present in `mapping_dict`. <br>&bull; Create `custom_dict` dictionary and `translated_dict` dictionaries from `gpt_mapping` in `gaming.yaml`. <br>&bull; Create `phon_dict` dictionary from `phon_mapping` in `gaming.yaml`. |\n",
    "| 2. | [Apply eng_to_ipa](#eng2ipa_gpt) | &bull; Apply phonemization on words in GPT corpus, which are detected as English by polyglot <br>&bull; Objectives are to analyse words that are phonemized (which should be English words in theory) and unphonemized words (which should be non-English words). |\n",
    "| 3. | [Analyse Unphonemized Words](#unphon_analysis) | &bull; Check if any proper English words that are not phonemized by eng_to_ipa. <br>&bull; Remove unphonemized English words if English words are found in `gpt_mapping`. <br>&bull; Create mapping (i.e. eng2ipa_mapping) to map English to combination of proper English words to replicate proper pronunciation of English word  e.g. twinking -> \"twin\" + \"king\". <br>&bull; If combining words fail to replicate proper pronunication, create mapping (i.e. `phon_mapping`) to map English words to customized phonemes based on CMU English vocubulary (on a best effort basis). |\n",
    "| 4. | [Analyse phonemized Words](#phon_analysis) | &bull; Check if Malay words are incorrectly phonemized by eng_to_ipa. <br>>&bull; Update list of Malay words.that are incorrectly phonemized (i.e. `wrong_malay`). |\n",
    "| 5. | [Update Mapping](#update_mapping) | &bull; Update `eng2ipa_mapping` with `mapping` in `gaming.yaml` as not all English gaming terms were utilized in our current GPT corpus. <br>&bull; Update `eng2ipa_mapping` with `gpt_mapping`, which contains both customized/modified English words; and translated English words to Malay. <br>&bull; Update `phon_mapping` with gaming terms that aren't able to replicate proper prounication by combination of English words (if any). <br>&bull; Include `edge_mapping` for case sensitive gaming terms e.g. 'HoT' vs 'hot'. |\n",
    "| 6. | [Append Normalized Text for Phonemization](#append_norm_phon) | &bull; Append normalized GPT text transcript customized for phonemization to DataFrame. |\n",
    "| 7. | [Generate GPT Phonemes](#gpt_phonemes) | &bull; Generate different phonemes sets for GPT corpus. |\n",
    "| 8. | [Generate Text Manifest for GPT Phonemes](#gpt_text_manifest) | &bull; Generate text manifest for generated phoneme sets. |\n",
    "| 9. | [Generate Text Manifest for Testing](#gen_test_manifest) | &bull; Generate text manifest to test out list of gaming terms in `combined_gaming_terms.csv`. |\n",
    "\n",
    "Note that:\n",
    "\n",
    "- `gpt_mapping` contains both English translation to Malay and custom words that are not detected by polyglot e.g. '2.5D'. <br>&bull; We will update custom words in `gpt_mapping` to ensure that it can be phonemized by eng_to_ipa.\n",
    "- Whole process is similar to [Section 4: Generate GPT Mapping](#gen_gpt_mapping) except that we are creating a larger mapping (i.e. `eng2ipa_mapping`) that maps English word to combinations of proper English words instead of modifying the characters in English word to achieve proper pronunciation (best effort basis).\n",
    "- Although `init_mapping` maps English words to combinations of actual/proper English words, we will leverage on `mapping`, which covers all gaming terms, so as not to miss out any gaming terms.\n",
    "- Proper English words are defined as sub-words (e.g. 'vo') or actual English words that are found inside CMU English vocabulary.\n",
    "- **We will only use the keys in `mapping` to generate the mapping from scratch as these mappings were constructed specifically for Malaya VITS model; and therefore not applicable for phonemization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup\n",
    "\n",
    "Following additional python modules are required to run this jupyter notebook\n",
    "\n",
    "```translators==5.8.9\n",
    "eng_to_ipa==0.0.2\n",
    "bs4==0.0.1\n",
    "nemo_text_processing==0.2.2rc0\n",
    "herpetologist==0.0.9\n",
    "malaya==5.0\n",
    "malaya_speech==1.3.0.2\n",
    "omegaconf==2.3.0\n",
    "whisper-openai==1.0.0\n",
    "pandarallel==1.6.5\n",
    "polyglot==16.7.4\n",
    "PySastrawi==1.2.0\n",
    "```\n",
    "\n",
    "ICU (International Components for Unicode) is required by polyglot to operate. PyICU is a python extension implemented in C++ that wraps the C/C++ ICU library. Installation of PyICU on Ubuntu is via binary packages of ICU and PyICU:\n",
    "\n",
    "```\n",
    "sudo apt-get install pkg-conifg libicu-dev\n",
    "pip install --no-binary=:pyicu: pyicu\n",
    "```\n",
    "\n",
    "Please refer to [PyICU - PyPI](https://pypi.org/project/PyICU/) for installation method for other operating systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import eng_to_ipa\n",
    "from omegaconf import OmegaConf\n",
    "import sys\n",
    "from pandarallel import pandarallel\n",
    "import IPython.display as ipd\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "# Append `tts-melayu` folder to sys path\n",
    "sys.path.append(Path.cwd().parent.as_posix())\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from notebooks.src_gaming.gaming_utils import (\n",
    "    gen_filelist, normalize_gaming, append_ipa,\n",
    ")\n",
    "from notebooks.src_gaming.gaming_gpt_phon_utils import (\n",
    "    normalize_gpt_phon, expand_compound,\n",
    "    phonemize_gpt, create_dict, save_json,\n",
    "    extract_phon_from_mapping, keys_in_values,\n",
    "    keys_in_keys, gen_unphon_gaming, extract_phon_unphon,\n",
    "    check_word, extract_unique, extract_non_english,\n",
    "    compute_word_count, phon_eng2ipa_espeak, common_translated_gaming,\n",
    ")\n",
    "from src.vits.text.cleaners import english_cleaners2\n",
    "from notebooks.src_gaming.gaming_gpt_utils import normalize_gpt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configurable parameters in gaming.yaml\n",
    "cfg = OmegaConf.load(\"/home/ckabundant/Documents/tts-melayu/notebooks/src_gaming/gaming.yaml\")\n",
    "args = OmegaConf.load(cfg.paths.gpt_yaml_path)\n",
    "\n",
    "# Initialize pandarallel\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=cfg.general.num_workers)\n",
    "\n",
    "cfg.paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents <a id='home'></a>\n",
    "1. &nbsp;&nbsp; [Create mapping](#create_mapping)\n",
    "    - Section 1.1 &nbsp;&nbsp; [Create mapping_dict](#mapping_dict)\n",
    "    - Section 1.2 &nbsp;&nbsp; [Create edge_dict](#edge_dict)\n",
    "    - Section 1.3 &nbsp;&nbsp; [Create phon_mapping](#phon_dict)\n",
    "    - Section 1.4 &nbsp;&nbsp; [Extract Unphonemized Gaming Terms](#unphon_gaming)\n",
    "    - Section 1.5 &nbsp;&nbsp; [Update mapping_dict](#update_mapping_dict)\n",
    "    - Section 1.6 &nbsp;&nbsp; [Create gpt_dict](#gpt_dict)\n",
    "    - Section 1.7 &nbsp;&nbsp; [Create custom_dict and translated_dict](#custom_translated_dict)\n",
    "    - Section 1.8 &nbsp;&nbsp; [Get Phonemized Keys](#get_phon_keys)\n",
    "    - Section 1.9 &nbsp;&nbsp; [Get Unphonemized Keys](#get_unphon_keys)\n",
    "2. &nbsp;&nbsp; [Apply eng_to_ipa](#eng2ipa_gpt)\n",
    "3. &nbsp;&nbsp; [Analyse Unphonemized Words](#unphon_analysis)\n",
    "    - Section 3.1 &nbsp;&nbsp; [Expand compound words](#expand_compound)\n",
    "    - Section 3.2 &nbsp;&nbsp; [Phonemize expanded words](#phon_expand)\n",
    "    - Section 3.3 &nbsp;&nbsp; [Extract unphonemized words](#extract_unphon)\n",
    "    - Section 3.4 &nbsp;&nbsp; [Update dictionaries](#update_dict)\n",
    "4. &nbsp;&nbsp; [Analyse Phonemized Words](#phon_analysis)\n",
    "    - Section 4.1 &nbsp;&nbsp; [Extract Phonemized Words](#extract_en_phon)\n",
    "    - Section 4.2 &nbsp;&nbsp; [Identify Wrong Malay Words](#wrong_malay)\n",
    "5. &nbsp;&nbsp; [Generate gaming_gpt.yaml](#gen_gaming_yaml)\n",
    "    - Section 5.1 &nbsp;&nbsp; [Update custom_dict and translated_dict](#update_custom_translated)\n",
    "    - Section 5.2 &nbsp;&nbsp; [Combine All Mapping](#combine_all_mapping)\n",
    "6. &nbsp;&nbsp; [Append Normalized Text](#append_norm_phon)\n",
    "7. &nbsp;&nbsp; [Generate GPT Phonemes](#gpt_phonemes)\n",
    "    - Section 7.1 &nbsp;&nbsp; [Phonemize with eng_to_ipa](#phon_eng2ipa)\n",
    "    - Section 7.2 &nbsp;&nbsp; [espeak_ms](#espeak_ms)\n",
    "    - Section 7.3 &nbsp;&nbsp; [espeak_en_ms](#espeak_en_ms)\n",
    "    - Section 7.4 &nbsp;&nbsp; [eng2ipa_espeak_ms](#eng2ipa_espeak_ms)\n",
    "8. &nbsp;&nbsp; [Generate Text Manifest for GPT Phonemes](#gpt_text_manifest)\n",
    "    - Section 8.1 &nbsp;&nbsp; [Text Manifest (espeak_ms)](#espeak_ms_txt)\n",
    "    - Section 8.2 &nbsp;&nbsp; [Text Manifest (espeak_en_ms)](#espeak_en_ms_txt)\n",
    "    - Section 8.3 &nbsp;&nbsp; [Text Manifest (eng2ipa_espeak_ms)](#eng2ipa_espeak_ms_txt)\n",
    "    - Section 8.4 &nbsp;&nbsp; [Text Manifest (norm_phon)](#norm_phon_txt)\n",
    "9. &nbsp;&nbsp; [Generate Test Filelist](#gen_test_filelist)\n",
    "    - Section 9.1 &nbsp;&nbsp; [Text Manifest (gpt_char)](#gpt_char)\n",
    "    - Section 9.2 &nbsp;&nbsp; [Text Manifest (gpt_espeak_en)](#gpt_espeak_en)\n",
    "    - Section 9.3 &nbsp;&nbsp; [Text Manifest (gpt_eng2ipa)](#gpt_eng2ipa)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. &nbsp;&nbsp; [Create mapping](#home) <a id='create_mapping'></a>\n",
    "\n",
    "Before phonemization, we have to create a new mapping to normalize GPT corpus. Reasons being:\n",
    "\n",
    "1. We are not able to use the mapping that was used to generate the audio pairs since it maps to modified words. Characters are modified in modified words and therefore are no longer actual English words.\n",
    "2. Since espeak and eng_to_ipa are unlikely to phonemize these modified words properly, there is a need for us to create a new mapping (i.e. `eng2ipa_mapping`) to map English words to proper English words.\n",
    "\n",
    "Reasons for using eng_to_ipa to phonemize are as follows:\n",
    "\n",
    "1. As shown in earlier sections, polyglot is not able to differentiate between English words and corresponding Malay words that sounds similar e.g. 'active' vs 'aktif'.\n",
    "2. Therefore we use eng_to_ipa, which should be able to phonemize only English words i.e. non-English words will be tagged with asterick.\n",
    "\n",
    "Unfortunately, we have to scrutinize the phonemes generated by eng_to_ipa as phonemization is not perfect:\n",
    "\n",
    "1. If words are relatively new and not updated in CMU English vocabulary, these words will not be able to be phonemized e.g. 'gank' used in gaming.\n",
    "2. Malay words (like 'ada', 'yang', etc., which shouldn't be phonemized) were phonemized by eng_to_ipa in English tone e.g. 'ada' which should be pronounced as 'ah da' was phonemized to sound 'eh da'.\n",
    "3. eng_to_ipa do not handle heteronyms e.g. there is only 1 pronounciation for 'read' and is default to 'reed' sound.\n",
    "\n",
    "We will generate 5 mapping namely: `mapping_dict`, `edge_dict`, `gpt_dict`, `custom_dict`, `translated_dict` and `phon_dict`. `translated_dict` will be appended to `custom_dict` to form `eng2ipa_mapping`. We will generate `gaming_gpt.yaml` by combining `phon_dict`, `edge_dict` and `eng2ipa_mapping`. Steps are as follows:\n",
    "\n",
    "<style>\n",
    "    table {\n",
    "        width: 70%;\n",
    "        table-layout:fixed;\n",
    "    }\n",
    "    th:nth-child(1),\n",
    "    td:nth-child(1) {\n",
    "        width: 3%;\n",
    "    }\n",
    "    th:nth-child(2),\n",
    "    td:nth-child(2) {\n",
    "        width: 17%;\n",
    "    }\n",
    "    th:nth-child(3),\n",
    "    td:nth-child(3) {\n",
    "        width: 80%;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "| S/N | Step | Description | \n",
    "| -- | -- | -- |\n",
    "| 1. | [Create mapping_dict](#mapping_dict) | &bull; Amend `mapping` to cater for eng_to_ipa phonemization. |\n",
    "| 2. | [Create edge_dict](#edge_dict) | &bull; Amend `mapping` to cater for eng_to_ipa phonemization. |\n",
    "| 3. | [Create phon_mapping](#phon_dict) | &bull; Convert `phon_mapping` in `gaming.yaml` to dictionary. |\n",
    "| 4. | [Extract Unphonemized Gaming Terms](#unphon_gaming) | &bull; Extract `term` and `eng2ipa` columns from `combined_gaming_terms.csv` |\n",
    "| 5. | [Update mapping_dict](#update_mapping) | &bull; Update unphonemized gaming terms to `mapping_dict`. |\n",
    "| 6. | [Create gpt_dict](#gpt_dict) | &bull; Convert `gpt_mapping` and `phon_mapping` in `gaming.yaml` to dictionary. |\n",
    "| 7. | [Create custom_dict and translated_dict](#custom_translated_dict) | &bull; Split `gpt_dict` dictionary into `custom_dict` and `translated_dict` for further processing. |\n",
    "\n",
    "| 8. | [Get Phonemized Keys](#get_phon_keys) | &bull; Get list of phonemized keys for `custom_dict` and `translated_dict`. |\n",
    "| 9. | [Get Unphonemized Keys](#get_unphon_keys) | &bull; Get list of unphonemized keys (from eng_to_ipa) in `custom_dict`. |\n",
    "| 10. | [Generate gaming_gpt.yaml](#gen_gaming_yaml) | &bull; Generate `gaming_gpt.yaml` with `mapping_dict`, `edge_dict`, `custom_dict` and `translated_dict`. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 &nbsp;&nbsp; [Create mapping_dict](#home) <a id='mapping_dict'></a>\n",
    "\n",
    "We attempt to use `mapping` in `gaming.yaml` (total of 380 key-value pairs) as far as possible:\n",
    "\n",
    "1. Combine proper English words to replicate pronounciation.\n",
    "2. If words combination doesn't generate the proper pronunciation (slight deviation), we provide the customized phonemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert `mapping` OmegaConf dictionary to `mapping_dict` dictionary\n",
    "mapping_dict = OmegaConf.to_object(cfg.mapping)\n",
    "mapping_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 &nbsp;&nbsp; [Create edge_dict](#home) <a id='edge_dict'></a>\n",
    "\n",
    "Dictionary mapping words with specific case e.g. 'HoT' vers 'hot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_dict = {\n",
    "    \"DoT\": \"dee o tea\",\n",
    "    \"HoT\": \"H o tea\",\n",
    "    \"t-pose\": \"tea pos\",\n",
    "    \"T-pose\": \"tea pos\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 &nbsp;&nbsp; [Create phon_mapping](#home) <a id='phon_dict'></a>\n",
    "\n",
    "We leverage on `phon_mapping` in `gaming.yaml` by converting it to `phon_dict`; and compare our custom phonemes with phonemes generated by espeak-en.\n",
    "\n",
    "Our observations:\n",
    "\n",
    "1. Our custom phonemes are slightly different from that created via espeak-en. \n",
    "2. While we are not able to discern the correct phonemes, we assume espeak-en would be more accurate compared to our custom phonemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert `phon_mapping` to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert OmegaConf dictionary `phon_mapping` to dictionary\n",
    "phon_dict = OmegaConf.to_object(cfg.phon_mapping)\n",
    "\n",
    "for k, v in phon_dict.items():\n",
    "    print(f\"{k:<25} : {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom phonemes versus espeak-en phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare between custom phonemes with that generated by espeak-en\n",
    "df_phon = pd.DataFrame.from_dict(phon_dict, orient=\"index\")\n",
    "df_phon = df_phon.reset_index()\n",
    "df_phon.columns = [\"words\", \"custom_phon\"]\n",
    "\n",
    "# Append phonemes generated by eng_to_ipa  to DataFrame\n",
    "df_phon[\"espeak_en\"] = df_phon[\"words\"].parallel_map(english_cleaners2)\n",
    "df_phon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 &nbsp;&nbsp; [Extract Unphonemized Gaming Terms](#home) <a id='unphon_gaming'></a>\n",
    "\n",
    "Our observations:\n",
    "\n",
    "1. 260 unphonemized gaming list generated from `combined_gaming_terms.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unphonemized gaming list\n",
    "unphon_gaming_list = gen_unphon_gaming(cfg.paths.combined_path)\n",
    "print(len(unphon_gaming_list))\n",
    "\n",
    "unphon_gaming_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 &nbsp;&nbsp; [Update mapping_dict](#home) <a id='update_mapping_dict'></a>\n",
    "\n",
    "We will first phonemize unphonemized gaming terms after removing the hyphen. The final unphonemized gaming terms (together with its mapping) will be added to `mapping_dict`. Subsequently, phonemized gaming terms are removed from `mapping_dict`.\n",
    "\n",
    "Our observations:\n",
    "\n",
    "1. 260 gaming terms were unphonemized by eng_to_ipa large due to presence of hyphen.\n",
    "2. Of the 260 gaming terms, 86 unphonemized gaming terms were not found in init_mapping.\n",
    "3. Final unphonemized gaming terms consist of 50 terms that will require mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Identify gaming terms in unphon_gaming not found in mapping_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unphon_gaming_not_in_mapping = extract_unique([unphon_gaming_list, mapping_dict.keys()])\n",
    "unphon_gaming_not_in_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert unphon_gaming_not_in_mapping to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unphon = pd.DataFrame({\"words\": unphon_gaming_not_in_mapping})\n",
    "df_unphon[\"eng2ipa\"] = df_unphon[\"words\"].parallel_map(eng_to_ipa.convert)\n",
    "df_unphon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract final unphonemized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_unphon = extract_phon_unphon(df_unphon, \"unphon\")\n",
    "print(len(df_final_unphon))\n",
    "df_final_unphon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create `update_dict` and `update_phon`\n",
    "\n",
    "We create customized phonemes for words that cannot be represented by word combinations.\n",
    "\n",
    "Our observations:\n",
    "\n",
    "1. 36 key-value pair to update to `mapping_dict`.\n",
    "2. 13 key-value pair to update to `phon_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_dict = {\n",
    "    \"miniboss\": \"mini boss\",\n",
    "    \"poggers\": \"pork girls\",\n",
    "    \"button mashing\": \"button mash shing\",\n",
    "    \"aimbot\": \"aim bought\",\n",
    "    \"completionist\": \"completion nist\",\n",
    "    \"telegraphing\": \"telegraph fin\",\n",
    "    \"waifu\": \"wai fu\",\n",
    "    \"chiptune\": \"chip tune\",\n",
    "    \"integrated i/o\": \"integrated I O\",\n",
    "    \"zerging\": \"ze ging\",\n",
    "    \"anti rpg\": \"anti R pee gee\",\n",
    "    \"real time corruptor\": \"real time corrupt ter\",\n",
    "    \"minimap\": \"mini map\",\n",
    "    \"cpu versus cpu\": \"C pee you versus C pee you\",\n",
    "    \"leaderboard\": \"leader board\",\n",
    "    \"overwatch\": \"over watch\",\n",
    "    \"on disc dlc\": \"on disc D el C\",\n",
    "    \"cheevo\": \"chee vo\",\n",
    "    \"arena fps\": \"arena F pee ass\",\n",
    "    \"transmog\": \"trans mock\",\n",
    "    \"always on drm\": \"always on dee R em\",\n",
    "    \"team deathmatch\": \"team death match\",\n",
    "    \"bullshot\": \"bull shot\",\n",
    "    \"softlock\": \"soft lock\",\n",
    "    \"hitscan\": \"hit scan\",\n",
    "    \"duping\": \"dupe ping\",\n",
    "    \"sistering\": \"sister ring\",\n",
    "    \"permadeath\": \"perma death\",\n",
    "    \"animatic\": \"any matic\",\n",
    "    \"metastory\": \"meta story\",\n",
    "    \"deathmatch\": \"death match\",\n",
    "    \"waggle\": \"wag girl\",\n",
    "    \"nerfing\": \"nerve fin\",\n",
    "    \"superboss\": \"super boss\",\n",
    "    \"spamming\": \"spam ming\",\n",
    "    \"bot\": \"bought\",\n",
    "}\n",
    "\n",
    "update_phon = {\n",
    "    \"proc\": \"ˈpɹɑk\",\n",
    "    \"battler\": \"ˈbætəlɚ\",\n",
    "    \"newb\": \"ˈnoʊb\",\n",
    "    \"noob\": \"ˈnoʊb\",\n",
    "    \"laner\": \"ˈleɪnɚ\",\n",
    "    \"gank\": \"ˈɡæŋk\",\n",
    "    \"squish\": \"ˈskwɪʃ\",\n",
    "    \"telefrag\": \"ˈtɛliˈfɹæk\",\n",
    "    \"metroidvania\": \"ˈmɛtˌɹɔɪdˈvˈeɪniə\",\n",
    "    \"frag\": \"ˈfɹæk\",\n",
    "    \"freemium\": \"ˈfɹimiəm\",\n",
    "    \"influencer\": \"ˈɪnfluənsɚ\",\n",
    "    \"gibs\": \"ˈɡɪbz\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Update `mapping_dict`\n",
    "\n",
    "Our observations:\n",
    "\n",
    "1. 416 key-value pairs in mapping_dict (up from 380 key-value pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict.update(update_dict)\n",
    "print(len(mapping_dict))\n",
    "mapping_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Update `phon_dict`\n",
    "\n",
    "Our observations:\n",
    "\n",
    "1. 23 key-value pairs currently in `phon_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phon_dict.update(update_phon)\n",
    "\n",
    "# sort `phon_dict`\n",
    "phon_dict = dict(sorted(phon_dict.items()))\n",
    "print(len(phon_dict))\n",
    "phon_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Update `mapping_dict` based on phonemized keys\n",
    "\n",
    "Our observations:\n",
    "\n",
    "1. 215 keys in `mapping` can be phonemized by eng_to_ipa.\n",
    "2. Out of 215 keys, 170 keys will be removed.\n",
    "3. Of the remaining 45 keys, mapping for 16 keys will be updated, which are mainly dealing with numbers.\n",
    "4. Updated `mapping_dict` are left with 210 key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out list of phonemized keys to generate dictionary to remove relevant phonemized keys\n",
    "df_phon_mapping = extract_phon_from_mapping(mapping_dict, \"phon\")\n",
    "\n",
    "for row in df_phon_mapping.itertuples(index=False, name=None):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate delete dictionary to remove selected phonemized keys\n",
    "delete_list = [\n",
    "    \"act\",\n",
    "    \"ace\",\n",
    "    \"achievement\",\n",
    "    \"achievements\",\n",
    "    \"add on\",\n",
    "    \"adds\",\n",
    "    \"aiming down sights\",\n",
    "    \"animation priority\",\n",
    "    \"area\",\n",
    "    \"asymmetric gameplay\",\n",
    "    \"asynchronous gameplay\",\n",
    "    \"arcade\",\n",
    "    \"asset\",\n",
    "    \"badge\",\n",
    "    \"base\",\n",
    "    \"balancing\",\n",
    "    \"blacklist\",\n",
    "    \"buff\",\n",
    "    \"bug\",\n",
    "    \"camping\",\n",
    "    \"cartridge\",\n",
    "    \"casual\",\n",
    "    \"character\",\n",
    "    \"cheating\",\n",
    "    \"checkpoint\",\n",
    "    \"cinematic\",\n",
    "    \"clap\",\n",
    "    \"clapped\",\n",
    "    \"clapping\",\n",
    "    \"clicker\",\n",
    "    \"clicking\",\n",
    "    \"clipping\",\n",
    "    \"clock\",\n",
    "    \"clocked\",\n",
    "    \"clone\",\n",
    "    \"closed\",\n",
    "    \"clutch\",\n",
    "    \"combo\",\n",
    "    \"console\",\n",
    "    \"coyote\",\n",
    "    \"cracked\",\n",
    "    \"crowd control\",\n",
    "    \"crunch\",\n",
    "    \"cut in\",\n",
    "    \"damage\",\n",
    "    \"de\",\n",
    "    \"devolution\",\n",
    "    \"dialog\",\n",
    "    \"dialogue\",\n",
    "    \"difficulty\",\n",
    "    \"directional pad\",\n",
    "    \"diverse\",\n",
    "    \"digital rights management\",\n",
    "    \"dungeon\",\n",
    "    \"dynamic\",\n",
    "    \"early access\",\n",
    "    \"emergent game play\",\n",
    "    \"extra sensory perception cheats\",\n",
    "    \"fear of missing out\",\n",
    "    \"field of view\",\n",
    "    \"final\",\n",
    "    \"frame buffer\",\n",
    "    \"frames\",\n",
    "    \"gameplay\",\n",
    "    \"game mechanics\",\n",
    "    \"game sense\",\n",
    "    \"gamer\",\n",
    "    \"gamers\",\n",
    "    \"gating\",\n",
    "    \"god\",\n",
    "    \"grind\",\n",
    "    \"grinding\",\n",
    "    \"hate\",\n",
    "    \"heal\",\n",
    "    \"health\",\n",
    "    \"hit point\",\n",
    "    \"horde\",\n",
    "    \"hud\",\n",
    "    \"identity\",\n",
    "    \"idle\",\n",
    "    \"in\",\n",
    "    \"infinite\",\n",
    "    \"invasion\",\n",
    "    \"item\",\n",
    "    \"johns\",\n",
    "    \"joystick\",\n",
    "    \"juggernaut\",\n",
    "    \"juggling\",\n",
    "    \"jump\",\n",
    "    \"kill stealing\",\n",
    "    \"launch game\",\n",
    "    \"lets\",\n",
    "    \"localization\",\n",
    "    \"live service games\",\n",
    "    \"loot\",\n",
    "    \"match fixing\",\n",
    "    \"macro\",\n",
    "    \"magic\",\n",
    "    \"main\",\n",
    "    \"maxed out\",\n",
    "    \"micro\",\n",
    "    \"monetization\",\n",
    "    \"motion blur\",\n",
    "    \"mud\",\n",
    "    \"multiplayer\",\n",
    "    \"multiplier\",\n",
    "    \"no clip mode\",\n",
    "    \"nuke\",\n",
    "    \"odd ball\",\n",
    "    \"on disc\",\n",
    "    \"overpowered\",\n",
    "    \"out of bounds\",\n",
    "    \"pause\",\n",
    "    \"palette swap\",\n",
    "    \"peak\",\n",
    "    \"perks\",\n",
    "    \"pervasive game\",\n",
    "    \"physical\",\n",
    "    \"physics\",\n",
    "    \"point of no return\",\n",
    "    \"pixel\",\n",
    "    \"pog\",\n",
    "    \"pub\",\n",
    "    \"pug\",\n",
    "    \"pull\",\n",
    "    \"purchase\",\n",
    "    \"quest giver\",\n",
    "    \"radar\",\n",
    "    \"rage\",\n",
    "    \"ratio\",\n",
    "    \"ray tracing\",\n",
    "    \"reactivity\",\n",
    "    \"reboot\",\n",
    "    \"remake\",\n",
    "    \"rhythm game\",\n",
    "    \"runner\",\n",
    "    \"saved game\",\n",
    "    \"scaling\",\n",
    "    \"scuffed\",\n",
    "    \"secret level\",\n",
    "    \"sequence breaking\",\n",
    "    \"skins\",\n",
    "    \"smurf\",\n",
    "    \"storytelling\",\n",
    "    \"specialization\",\n",
    "    \"squeaker\",\n",
    "    \"status effects\",\n",
    "    \"status\",\n",
    "    \"snip\",\n",
    "    \"stream sniping\",\n",
    "    \"streaming\",\n",
    "    \"sweat\",\n",
    "    \"telegraph\",\n",
    "    \"tick\",\n",
    "    \"timed\",\n",
    "    \"title screen\",\n",
    "    \"tower dive\",\n",
    "    \"toxicity\",\n",
    "    \"turn based\",\n",
    "    \"underpowered\",\n",
    "    \"underworld\",\n",
    "    \"unlock\",\n",
    "    \"under levelled\",\n",
    "    \"ultimate\",\n",
    "    \"upgrade\",\n",
    "    \"virtual reality\",\n",
    "    \"wall bang\",\n",
    "    \"wall climb\",\n",
    "    \"whale\",\n",
    "    \"youtube bait\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate updated dictionary to combine actual English words to generate desired pronunciation\n",
    "updated_dict = {\n",
    "    \"2.5d graphics\": \"two point five dee graphics\",\n",
    "    \"2d graphics\": \"two dee graphics\",\n",
    "    \"3d graphics\": \"three dee graphics\",\n",
    "    \"8k resolution\": \"eight kay resolution\",\n",
    "    \"8 bit\": \"eight bit\",\n",
    "    \"16 bit\": \"sixteen bit\",\n",
    "    \"32 bit\": \"thirty two bit\",\n",
    "    \"64 bit\": \"sixty four bit\",\n",
    "    \"abandonware\": \"abandon ware\",\n",
    "    \"borderless fullscreen windowed\": \"border less full screen windowed\",\n",
    "    \"indie game\": \"in dee game\",\n",
    "    \"kd ratio\": \"kay dee ratio\",\n",
    "    \"lp\": \"lets play\",\n",
    "    \"noclip mode\": \"no clip mode\",\n",
    "    \"vaporware\": \"vapor ware\",\n",
    "    \"wasd keys\": \"w a s d keys\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update mapping_dict with updated_dict\n",
    "mapping_dict.update(updated_dict)\n",
    "\n",
    "# Remove irrelevant keys based on delete_list\n",
    "for key in delete_list:\n",
    "    mapping_dict.pop(key)\n",
    "\n",
    "# Sort by key in ascending order\n",
    "mapping_dict = dict(sorted(mapping_dict.items()))\n",
    "mapping_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Update `mapping_dict` based on unphonemized keys\n",
    "\n",
    "Our observations:\n",
    "\n",
    "1. 210 key-value pairs with unphonemized key were detected.\n",
    "2. We updated 51 key-value pairs to ensure that English words are mapped to proper word combination.\n",
    "3. Certain words are not able to be represented with word combinations (e.g. 'downloadable', 'mudflation', 'monetization', etc.). We will generate the corresponding customized phonemes; and remove from `mapping_dict`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out list of unphonemized keys to generate dictionary to remove relevant phonemized keys\n",
    "df_unphon_mapping = extract_phon_from_mapping(mapping_dict, \"unphon\")\n",
    "\n",
    "for row in df_unphon_mapping.itertuples(index=False, name=None):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate updated dictionary to combine actual English words to generate desired pronunciation\n",
    "updated_dict_1 = {\n",
    "    \"aggro\": \"ag grow\",\n",
    "    \"control pt\": \"control point\",\n",
    "    \"cooldown\": \"cool down\",\n",
    "    \"co op\": \"cooperative game play\",\n",
    "    \"corruptor\": \"corrupt ter\",\n",
    "    \"cranking 90s\": \"cranking nineties\",\n",
    "    \"ctf\": \"capture the flag\",\n",
    "    \"dbno\": \"down but not out\",\n",
    "    \"debuff\": \"dee buff\",\n",
    "    \"demake\": \"dee make\",\n",
    "    \"destructible\": \"destructable\",\n",
    "    \"dpm\": \"damage per minute\",\n",
    "    \"drm\": \"digital right management\",\n",
    "    \"emulator\": \"E mule later\",\n",
    "    \"esports\": \"E sports\",\n",
    "    \"exp\": \"experience point\",\n",
    "    \"fangame\": \"fan game\",\n",
    "    \"foozle\": \"foo ze\",\n",
    "    \"fotm\": \"flavor of the month\",\n",
    "    \"gimp\": \"gym\",\n",
    "    \"git gud\": \"git good\",\n",
    "    \"goty\": \"game of the year\",\n",
    "    \"griefer\": \"grief fur\",\n",
    "    \"hitbox\": \"hit box\",\n",
    "    \"iap\": \"in app purchase\",\n",
    "    \"iframes\": \"eye frames\",\n",
    "    \"microtransaction\": \"micro transaction\",\n",
    "    \"pentakill\": \"penta kill\",\n",
    "    \"playthrough\": \"play through\",\n",
    "    \"pulc\": \"premium unlockable content\",\n",
    "    \"qa\": \"queue A\",\n",
    "    \"qq\": \"queue queue\",\n",
    "    \"ragedoll\": \"rage doll\",\n",
    "    \"ragequit\": \"rage quit\",\n",
    "    \"remorting\": \"re mort ting\",\n",
    "    \"rngesus\": \"R and jesus\",\n",
    "    \"roguelike\": \"rogue like\",\n",
    "    \"roguelite\": \"rogue light\",\n",
    "    \"save scumming\": \"save scum ming\",\n",
    "    \"shovelware\": \"shovel ware\",\n",
    "    \"speedrunning\": \"speed running\",\n",
    "    \"theorycraft\": \"theory craft\",\n",
    "    \"thumbstick\": \"thumb stick\",\n",
    "    \"touchscreen\": \"touch screen\",\n",
    "    \"trickjump\": \"trick jump\",\n",
    "    \"vrr\": \"variable refresh rate\",\n",
    "    \"walkthrough\": \"walk through\",\n",
    "    \"wallbang\": \"wall bang\",\n",
    "    \"wallhack\": \"wall hack\",\n",
    "    \"wp\": \"W pee\",\n",
    "    \"xp\": \"experience point\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update `mapping_dict` with `updated_dict_1`\n",
    "mapping_dict.update(updated_dict_1)\n",
    "\n",
    "# Sort by key in ascending order\n",
    "mapping_dict = dict(sorted(mapping_dict.items()))\n",
    "mapping_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 &nbsp;&nbsp; [Create gpt_dict](#home) <a id='gpt_dict'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert `gpt_mapping` OmegaConf dictionary to `gpt_dict` dictionary\n",
    "gpt_dict = OmegaConf.to_object(cfg.gpt_mapping)\n",
    "gpt_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 &nbsp;&nbsp; [Create custom_dict and translated_dict](#home) <a id='custom_translated_dict'></a>\n",
    "\n",
    "`translated_mapping` starts from 'abilities' in `gpt_mapping`. Convert `gpt_dict` keys to list and determine the index for 'abilities'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate `gpt_dict` to `custom_dict` and `translated_dict`\n",
    "custom_dict, translated_dict = create_dict(gpt_dict)\n",
    "custom_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove words in `translated_dict` that are found in values of `mapping_dict`\n",
    "\n",
    "Rationale is to avoid normalized gaming terms to be amended by `eng2ipa_mapping` (i.e. combined).\n",
    "\n",
    "Our observations:\n",
    "\n",
    "1. 'counter terrorist', 'flavor', 'lets', 'minute', 'variable', 'year' are found in the values of `mapping_dict`.\n",
    "2. Keys in `translated_dict` are different from keys in `custom_dict`.\n",
    "3. Keys that are common in `custom_dict` and `mapping_dict` are '2.5D', '2D', '3D', '4K', '8K', and 'wasd'.\n",
    "4. Similarly, keys in `translated_dict` are different from keys in `mapping_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for common keys in `custom_dict` and `translated_dict`\n",
    "keys_in_keys(custom_dict, translated_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for common keys in `custom_dict` and `mapping_dict`\n",
    "keys_in_keys(custom_dict, mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for common keys in `translated_dict` and `mapping_dict`\n",
    "keys_in_keys(translated_dict, mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = \"product\"\n",
    "text = \"Games as a Product\"\n",
    "\n",
    "pattern = rf\"^{re.escape(term)}$|\\s+{re.escape(term)}\\s+|^{re.escape(term)} | {re.escape(term)}$\"\n",
    "if len(re.findall(pattern, text, flags=re.IGNORECASE)) > 0:\n",
    "    print(\"True\")\n",
    "\n",
    "# re.findall(pattern, text, flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for common keys in `translated_dict` and gaming terms\n",
    "remove_list = common_translated_gaming(cfg.paths.combined_path, translated_dict)\n",
    "print(remove_list)\n",
    "\n",
    "# Remove above keys in translated_dict\n",
    "for key in remove_list:\n",
    "    translated_dict.pop(key)\n",
    "\n",
    "translated_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find list of keys in translated_dict that are also found in value of mapping_dict\n",
    "remove_key = keys_in_values(translated_dict, mapping_dict)\n",
    "print(remove_key)\n",
    "\n",
    "# Remove above keys in translated_dict\n",
    "for key in remove_key:\n",
    "    translated_dict.pop(key)\n",
    "\n",
    "translated_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove keys in `mapping_dict`, `custom_dict`, and `translated_dict` that are also present in `phon_dict`\n",
    "\n",
    "Our observations:\n",
    "\n",
    "1. 8 keys in `mapping_dict` are also present in `phon_dict`: 'downloadable', 'gimp', 'judder', 'mudlflation', 'noob', 'transmogrification', 'unlockable', 'unlocks'.\n",
    "2. 203 key-value pairs are left in `mapping_dict`.\n",
    "3. No keys in either `custom_dict` or `translated_dict` are found in `phon_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify list of keys that are present in `mapping_dict` and `phon_dict`\n",
    "remove_keys = keys_in_keys(mapping_dict, phon_dict)\n",
    "print(remove_keys)\n",
    "\n",
    "# Remove duplicated keys\n",
    "for key in remove_keys:\n",
    "    mapping_dict.pop(key)\n",
    "\n",
    "mapping_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify list of keys that are present in `custom_dict` and `phon_dict`\n",
    "remove_key = keys_in_keys(custom_dict, phon_dict)\n",
    "remove_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify list of keys that are present in `translated_dict` and `phon_dict`\n",
    "remove_key = keys_in_keys(translated_dict, phon_dict)\n",
    "remove_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 &nbsp;&nbsp; [Get Phonemized Keys](#home) <a id='get_phon_keys'></a>\n",
    "\n",
    "Our observations:\n",
    "\n",
    "1. 17 keys in `custom_dict` are phonemized.\n",
    "2. Phonemes for 'sci fi' (ˈɛsˈsiˈaɪ fi) implies 'sci fi' are pronounced separately and are incorrect.\n",
    "3. 721 keys in `translated_dict` are phonemized. No action required as 721 keys are proper English words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Phonemized key-value pair for custom_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_phon_keys = [key for key in custom_dict.keys() if eng_to_ipa.convert(key)[-1] != \"*\"]\n",
    "\n",
    "for key in custom_phon_keys:\n",
    "    print(f\"{key:<15} : {custom_dict[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Phonemized keys for translated_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_phon_keys = [key for key in translated_dict.keys() if eng_to_ipa.convert(key)[-1] != \"*\"]\n",
    "\n",
    "for key in translated_phon_keys:\n",
    "    print(f\"{key:<15} : {translated_dict[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 &nbsp;&nbsp; [Get Unphonemized Keys](#home) <a id='get_unphon_keys'></a>\n",
    "\n",
    "Our observations:\n",
    "\n",
    "1. 11 keys in `custom_dict` are unphonemized.\n",
    "2. 13 keys in `translated_dict` are unphonemized i.e. only 13 out of 734 English words in `translation_dict` are unphonemized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unphonemized keys for custom_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_unphon_keys = [key for key in custom_dict.keys() if eng_to_ipa.convert(key)[-1] == \"*\"]\n",
    "\n",
    "for key in custom_unphon_keys:\n",
    "    print(f\"{key:<15} : {custom_dict[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unphonemized keys for translated_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_unphon_keys = [key for key in translated_dict.keys() if eng_to_ipa.convert(key)[-1] == \"*\"]\n",
    "\n",
    "for key in translated_unphon_keys:\n",
    "    print(f\"{key:<20} : {translated_dict[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. &nbsp;&nbsp; [Apply eng_to_ipa](#home) <a id='eng2ipa_gpt'></a>\n",
    "\n",
    "Phonemize all 1833 English words detected by polyglot in GPT corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_word = pd.read_csv(cfg.paths.gpt_en_path)\n",
    "df_en_word[\"eng2ipa\"] = df_en_word[\"words\"].parallel_map(eng_to_ipa.convert)\n",
    "df_en_word.to_csv(cfg.paths.gpt_en_path, index=False)\n",
    "df_en_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. &nbsp;&nbsp; [Analyse Unphonemized Words](#home) <a id='unphon_analysis'></a>\n",
    "\n",
    "We noted that words that are not phonemized are as follows:\n",
    "\n",
    "<style>\n",
    "    table {\n",
    "        width: 70%;\n",
    "        table-layout:fixed;\n",
    "    }\n",
    "    th:nth-child(1),\n",
    "    td:nth-child(1) {\n",
    "        width: 3%;\n",
    "    }\n",
    "    th:nth-child(2),\n",
    "    td:nth-child(2) {\n",
    "        width: 17%;\n",
    "    }\n",
    "    th:nth-child(3),\n",
    "    td:nth-child(3) {\n",
    "        width: 80%;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "| S/N | Unphonemized Word | Examples |\n",
    "| -- | -- | -- |\n",
    "| 1. | Compound words linked by hyphen | action-adventure, role-playing, no-scope, etc. |\n",
    "| 2. | Malay words | aktif, evolusi, tangan, etc. |\n",
    "| 3. | Compound words without white space | artbook, microtransactions, roguelike, etc. |\n",
    "| 4. | Abbreviations with slash are not phonemized | i/o , k/d, etc. |\n",
    "\n",
    "Hence, we took the following approaches:\n",
    "\n",
    "1. Expand compound words since individual word making up the compound word can be phonemized separately; and remove any duplicates.\n",
    "2. Phonemize the expanded words to extract the unphonemized words again for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 &nbsp;&nbsp; [Expand compound words](#home) <a id='expand_compound'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand compound words by spliting the compound word by hyphen or slash.\n",
    "df_expand = expand_compound(df_en_word[\"words\"])\n",
    "df_expand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 &nbsp;&nbsp; [Phonemize expanded words](#home) <a id='phon_expand'></a>\n",
    "\n",
    "Our observations:\n",
    "\n",
    "1. After expanding compound words and removing duplicates, number of English words found in GPT corpus reduce from 1833 to 1806 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phonemize English words detected by polyglot and append to DataFrame\n",
    "df_expand[\"eng2ipa\"] = df_expand[\"words\"].parallel_map(eng_to_ipa.convert)\n",
    "df_expand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 &nbsp;&nbsp; [Extract unphonemized words](#home) <a id='extract_unphon'></a>\n",
    "\n",
    "Our observations:\n",
    "\n",
    "1. 433 unphonemized words detected out of 1806 English words used in GPT corpus.\n",
    "2. Removing unphonemized words that are found in `mapping_dict`, `custom_dict`, `translated_dict`, and `phon_dict`, we have total of 318 words left, most of which are Malay words.\n",
    "3. 11 unphonemized words that are English: 'tps', 'pokemon', 'modding', 'platformer', 'parkour', 'platformers', 'scolling', 'scumming', 'tump', 'haah', and 'mvp'.\n",
    "\n",
    "All 11 unphonemeized words except for 'haah' are updated to `custom_dict` while 'haah' is updated to `phon_dict`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter out unphonemized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expand_unphon = extract_phon_unphon(df_expand, \"unphon\")\n",
    "df_expand_unphon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract unique words not in `mapping_dict`, `custom_dict`, `translated_dict` and `phon_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unphon_en = df_expand_unphon[\"words\"].to_list()\n",
    "unique_unphon_en = extract_unique(\n",
    "    [unphon_en, mapping_dict.keys(), custom_dict.keys(), translated_dict.keys(), phon_dict.keys()]\n",
    ")\n",
    "unique_unphon_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 &nbsp;&nbsp; [Update dictionaries](#home) <a id='update_dict'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update 10 unphonemized English words to `custom_dict`\n",
    "update_dict = {\n",
    "    \"tps\": \"tee pee ass\",\n",
    "    \"pokemon\": \"poke ki mon\",\n",
    "    \"modding\": \"mod ding\",\n",
    "    \"platformer\": \"platform mer\",\n",
    "    \"platformers\": \"platform mers\",\n",
    "    \"parkour\": \"park call\",\n",
    "    \"scrolling\": \"scroll ling\",\n",
    "    \"scumming\": \"scum ming\",\n",
    "    \"tump\": \"thump\",\n",
    "    \"mvp\": \"em vee pee\",\n",
    "}\n",
    "\n",
    "update_phon = {\"haah\": \"ˈhɑɑ\"}\n",
    "\n",
    "# Update `custom_dict`\n",
    "custom_dict.update(update_dict)\n",
    "\n",
    "# Update `phon_dict`\n",
    "phon_dict.update(update_phon)\n",
    "\n",
    "print(phon_dict)\n",
    "custom_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. &nbsp;&nbsp; [Analyse Phonemized Words](#home) <a id='phon_analysis'></a>\n",
    "\n",
    "Identify Malay words that are phonemized by eng_to_ipa. Steps taken:\n",
    "\n",
    "1. Extract phonemized words.\n",
    "2. Identify Malay words that are incorrectly phonemized by eng_to_ipa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 &nbsp;&nbsp; [Extract Phonemized Words](#home) <a id='extract_en_phon'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out phonemized words\n",
    "df_expand_phon = extract_phon_unphon(df_expand)\n",
    "df_expand_phon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 &nbsp;&nbsp; [Identify Wrong Malay Words](#wrong_malay)\n",
    "\n",
    "We apply phonemization on all words in GPT corpus and identify Malay words that are incorrectly phonemized by eng_to_ipa\n",
    "\n",
    "Our observations:\n",
    "\n",
    "1. Most English words are used in gaming terms. Hence, there is no need for further translation to Malay.\n",
    "2. 76 Malay words were wrongly phonemized by eng_to_ipa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gpt corpus as DataFrame\n",
    "df_gpt = pd.read_csv(cfg.paths.gpt_csv_path)\n",
    "\n",
    "# Extract phonemized words from entire GPT corpus\n",
    "phon_gpt = extract_non_english(df_gpt)\n",
    "phon_gpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Identify Malay words that are incorrectly phonemized by eng_to_ipa and update list to `gaming_gpt.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phon_malay = [\n",
    "    \"a\",\n",
    "    \"ada\",\n",
    "    \"adalah\",\n",
    "    \"agar\",\n",
    "    \"ajar\",\n",
    "    \"alam\",\n",
    "    \"aman\",\n",
    "    \"antar\",\n",
    "    \"baca\",\n",
    "    \"bang\",\n",
    "    \"beli\",\n",
    "    \"berjaya\",\n",
    "    \"bila\",\n",
    "    \"bos\",\n",
    "    \"cara\",\n",
    "    \"cuba\",\n",
    "    \"dah\",\n",
    "    \"dan\",\n",
    "    \"daya\",\n",
    "    \"di\",\n",
    "    \"dia\",\n",
    "    \"fantastik\",\n",
    "    \"gila\",\n",
    "    \"hari\",\n",
    "    \"ia\",\n",
    "    \"ikon\",\n",
    "    \"industri\",\n",
    "    \"isu\",\n",
    "    \"jaya\",\n",
    "    \"je\",\n",
    "    \"kan\",\n",
    "    \"kadar\",\n",
    "    \"kasi\",\n",
    "    \"kat\",\n",
    "    \"kaya\",\n",
    "    \"ke\",\n",
    "    \"kempen\",\n",
    "    \"kira\",\n",
    "    \"kita\",\n",
    "    \"kot\",\n",
    "    \"kredit\",\n",
    "    \"la\",\n",
    "    \"lain\",\n",
    "    \"lama\",\n",
    "    \"lupa\",\n",
    "    \"mahal\",\n",
    "    \"mana\",\n",
    "    \"masa\",\n",
    "    \"mata\",\n",
    "    \"maya\",\n",
    "    \"minda\",\n",
    "    \"minit\",\n",
    "    \"mula\",\n",
    "    \"muzik\",\n",
    "    \"naik\",\n",
    "    \"ni\",\n",
    "    \"okey\",\n",
    "    \"performa\",\n",
    "    \"peta\",\n",
    "    \"pintar\",\n",
    "    \"polis\",\n",
    "    \"ronda\",\n",
    "    \"saling\",\n",
    "    \"sama\",\n",
    "    \"sana\",\n",
    "    \"servis\",\n",
    "    \"siri\",\n",
    "    \"tac\",\n",
    "    \"tak\",\n",
    "    \"tanya\",\n",
    "    \"tim\",\n",
    "    \"tu\",\n",
    "    \"wang\",\n",
    "    \"yang\",\n",
    "    \"zaman\",\n",
    "    \"zona\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. &nbsp;&nbsp; [Generate gaming_gpt.yaml](#home) <a id='gen_gaming_yaml'></a>\n",
    "\n",
    "We update our mapping dictionaries i.e. `phon_dict`, `edge_dict`, `mapping_dict`, `custom_dict`, `translated_dict` and `phon_malay` before saving as `gaming_gpt.yaml`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 &nbsp;&nbsp; [Update custom_dict and translated_dict](#home) <a id='update_custom_translated'></a>\n",
    "\n",
    "Actions to be taken:\n",
    "\n",
    "<style>\n",
    "    table {\n",
    "        width: 70%;\n",
    "        table-layout:fixed;\n",
    "    }\n",
    "    th:nth-child(1),\n",
    "    td:nth-child(1) {\n",
    "        width: 3%;\n",
    "    }\n",
    "    th:nth-child(2),\n",
    "    td:nth-child(2) {\n",
    "        width: 27%;\n",
    "    }\n",
    "    th:nth-child(3),\n",
    "    td:nth-child(3) {\n",
    "        width: 70%;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "| S/N | Action | Rationale | \n",
    "| -- | -- | -- |\n",
    "| 1. | Create words combination for unphonemized keys in `custom_dict` | &bull; Update words combination for '8K', 'geocaching', 'wasd', and 'xbox'. |\n",
    "| 2. | Remove phonemized keys in `custom_dict` | &bull; `custom_dict` is developed to modify English words for Malaya VITS model inferencing. <br>&bull; Therefore, there is no need to keep English words that are able to be phonemized by eng_to_ipa. |\n",
    "| 3. | Create customized phonemes | &bull; Create customized phonemes for 'mods', 'sci fi', 'scoped', 'whiffs' and remove these words in `custom_mapping`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Update `custom_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dict.update({\"8K\": \"eight kay\", \"geocaching\": \"geo caching\", \"wasd\": \"w a s d\", \"xbox\": \"axe box\"})\n",
    "custom_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove phonemized `custom_dict` keys found in `custom_phon_keys` list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in custom_phon_keys:\n",
    "    custom_dict.pop(key)\n",
    "\n",
    "custom_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Update `phon_dict` and `custom_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create customized phonemes\n",
    "update_phon = {\n",
    "    \"mods\": \"ˈmɔdz\",\n",
    "    \"sci fi\": \"ˌsaɪˈfaɪ\",\n",
    "    \"scoped\": \"ˈskoʊpt\",\n",
    "    \"whiffs\": \"ˈwɪfs\",\n",
    "}\n",
    "\n",
    "# Update `phon_dict`\n",
    "phon_dict.update(update_phon)\n",
    "\n",
    "# Remove words with created phonemes from `custom_dict`\n",
    "for key in update_phon.keys():\n",
    "    # 'sci fi' has been removed earlier under custom_phon_keys\n",
    "    if key != \"sci fi\":\n",
    "        custom_dict.pop(key)\n",
    "\n",
    "print(len(custom_dict))\n",
    "custom_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save `custom_dict`, `translated_dict` and `phon_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(custom_dict, cfg.paths.custom_dict_path)\n",
    "save_json(translated_dict, cfg.paths.translated_dict_path)\n",
    "save_json(phon_dict, cfg.paths.phon_dict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 &nbsp;&nbsp; [Combine All Mapping](#home) <a id='combine_all_mapping'></a>\n",
    "Steps as follows:\n",
    "\n",
    "1. Combine `phon_dict`, `mapping_dict`, `edge_dict`, `custom_dict`, `translated_dict` to `gaming_gpt_dict`.\n",
    "2. Convert `gaming_gpt_dict` to OmegaConfig Dictionary `gaming_gpt_mapping`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate `gaming_gpt_mapping`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dictionaries\n",
    "phon_dict = dict(sorted(phon_dict.items()))\n",
    "mapping_dict = dict(sorted(mapping_dict.items()))\n",
    "custom_dict = dict(sorted(custom_dict.items()))\n",
    "translated_dict = dict(sorted(translated_dict.items()))\n",
    "phon_malay = sorted(phon_malay)\n",
    "\n",
    "gaming_gpt_dict = {\n",
    "    \"phon_mapping\": phon_dict,\n",
    "    \"mapping\": mapping_dict,\n",
    "    \"edge_mapping\": edge_dict,\n",
    "    \"custom_mapping\": custom_dict,\n",
    "    \"translated_mapping\": translated_dict,\n",
    "    \"phon_malay\": phon_malay,\n",
    "}\n",
    "\n",
    "gaming_gpt_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert to OmegaConfig dictionary and save as `gaming_gpt.yaml` yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaming_gpt_mapping = OmegaConf.create(gaming_gpt_dict)\n",
    "print(OmegaConf.to_yaml(gaming_gpt_mapping))\n",
    "\n",
    "# Save as yaml file\n",
    "OmegaConf.save(gaming_gpt_mapping, cfg.paths.gpt_yaml_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. &nbsp;&nbsp; [Append Normalized Text for Phonemization](#home) <a id='append_norm_phon'></a>\n",
    "\n",
    "We have updated `eng2ipa_mapping` with gaming terms and relevant English words from GPT corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpt[\"norm_phon\"] = df_gpt[\"text\"].parallel_map(lambda x: normalize_gpt_phon(x, cfg.paths.gpt_yaml_path))\n",
    "df_gpt.to_csv(cfg.paths.gpt_csv_path, index=False)\n",
    "df_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df_gpt.loc[:, [\"norm_gpt\", \"norm_phon\"]].itertuples(index=False, name=None):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. &nbsp;&nbsp; [Generate GPT Phonemes](#home) <a id='gpt_phonemes'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 &nbsp;&nbsp; [Phonemize with eng_to_ipa](#home) <a id='phon_eng2ipa'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpt = phonemize_gpt(df_gpt, \"eng2ipa\")\n",
    "df_gpt.to_csv(cfg.paths.gpt_csv_path, index=False)\n",
    "df_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df_gpt.loc[:, [\"text\", \"eng2ipa\"]].itertuples(index=False, name=None):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 &nbsp;&nbsp; [espeak_ms](#home) <a id='espeak_ms'></a>\n",
    "\n",
    "We perform phonemization of GPT corpus via espeak-ms for the purpose of timing the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpt = phonemize_gpt(df_gpt, \"espeak_ms\")\n",
    "df_gpt.to_csv(cfg.paths.gpt_csv_path, index=False)\n",
    "df_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df_gpt.loc[:, [\"id\", \"norm_phon\", \"espeak_ms\"]].itertuples(index=False, name=None):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 &nbsp;&nbsp; [espeak_en_ms](#home) <a id='espeak_en_ms'></a>\n",
    "\n",
    "We attempt to apply phonemization only on English words via espeak-en and remaining words (largely Malay) via espeak-ms. This involves iterating through each word in transcript and use polyglot to determine whether it is English word. Phonemization via espeak-en on English words should produce phonemes that captures accurately prosdy and intonation of English speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpt = phonemize_gpt(df_gpt, \"espeak_en_ms\", phon_mapping=list(args.phon_mapping))\n",
    "df_gpt.to_csv(cfg.paths.gpt_csv_path, index=False)\n",
    "df_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df_gpt.loc[:, [\"id\", \"norm_phon\", \"espeak_en_ms\"]].itertuples(index=False, name=None):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 &nbsp;&nbsp; [eng2ipa_espeak_ms](#home) <a id='eng2ipa_espeak_ms'></a>\n",
    "\n",
    "We are using eng_to_ipa and `phon_mapping` to phonemize English words and espeak-ms to phonemize Malay words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpt = phonemize_gpt(df_gpt, \"eng2ipa_espeak_ms\", phon_mapping=list(args.phon_mapping))\n",
    "df_gpt.to_csv(cfg.paths.gpt_csv_path, index=False)\n",
    "df_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df_gpt.loc[:, [\"id\", \"norm_phon\", \"eng2ipa_espeak_ms\"]].itertuples(index=False, name=None):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. &nbsp;&nbsp; [Generate Text Manifest for GPT Phonemes](#home) <a id='gpt_text_manifest'></a>\n",
    "\n",
    "Generate text manifest files for normalized GPT corpus from `norm_phon` columns; and for 3 different versions of phonemes (i.e. espeak_ms, espeak_en_ms, and eng2ipa_espeak_ms) under `gpt` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 &nbsp;&nbsp; [Text Manifest (espeak_ms)](#home) <a id='espeak_ms_txt'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_filelist(df_gpt, cfg.paths.gaming_dir, \"gpt_ms\", \"espeak_ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 &nbsp;&nbsp; [Text Manifest (espeak_en_ms)](#home) <a id='espeak_en_ms_txt'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_filelist(df_gpt, cfg.paths.gaming_dir, \"gpt_en_ms\", \"espeak_en_ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 &nbsp;&nbsp; [Text Manifest (eng2ipa_espeak_ms)](#home) <a id='eng2ipa_espeak_ms_txt'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_filelist(df_gpt, cfg.paths.gaming_dir, \"gpt_eng2ipa_ms\", \"eng2ipa_espeak_ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 &nbsp;&nbsp; [Text Manifest (norm_phon)](#home) <a id='norm_phon_txt'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_filelist(df_gpt, cfg.paths.gaming_dir, \"norm_phon\", \"norm_phon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. &nbsp;&nbsp; [Generate Test Filelist](#home) <a id='gen_test_manifest'></a>\n",
    "\n",
    "We have trained VITS model checkpoints (minimum 100 epochs) using raw words (i.e. normalized gaming terms), phonemes generated by espeak-en and phonemes generated by eng_to_ipa. We intend to generate text manifest based on the gaming list to test out the effectiveness of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 &nbsp;&nbsp; [Text Manifest (gpt_char)](#home) <a id='gpt_char'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.read_csv(cfg.paths.combined_path)\n",
    "df_combined[\"norm_term\"] = df_combined[\"term\"].parallel_map(lambda x: normalize_gpt_phon(x, cfg.paths.gpt_yaml_path))\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate `gpt_char.txt` filelist under `gpt_char` folder in `gaming` folder\n",
    "gen_filelist(df_combined, cfg.paths.gaming_dir, \"gpt_char\", \"norm_term\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 &nbsp;&nbsp; [Text Manifest (gpt_espeak_en)](#home) <a id='gen_espeak_en_filelist'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = append_ipa(df_combined, \"ipa_norm_en\")\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate `gpt_espeak_en.txt` filelist under `gpt_espeak_en` folder in `gaming` folder\n",
    "gen_filelist(df_combined, cfg.paths.gaming_dir, \"gpt_espeak_en\", \"ipa_norm_en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 &nbsp;&nbsp; [Text Manifest (gpt_eng2ipa)](#home) <a id='gpt_eng2ipa'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = phonemize_gpt(df_combined, \"eng2ipa_espeak_ms\", \"norm_term\", args.phon_mapping)\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate `gpt_eng2ipa.txt` filelist under `gpt_eng2ipa_en` folder in `gaming` folder\n",
    "gen_filelist(df_combined, cfg.paths.gaming_dir, \"gpt_eng2ipa\", \"eng2ipa_espeak_ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
